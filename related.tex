\section{Related Work}\label{sec:related}
The idea that all level-3 BLAS operations can be built 
on top of 
a high-performance GEMM implementation was first proposed
in~\cite{gemmbased1,gemmbased2}.
The GotoBLAS library~\cite{gotoblas} and its successor OpenBLAS~\cite{openblas},
are instantiated based on this insight.
So optimizing GEMM has always been the central task in developing
dense linear algebra software.
The GEMM optimization has two aspects.
One is developing fast computation kernels and
the other is choosing a proper overall workload-partitioning  strategy
to optimize memory access.

%% Broadly speaking, there are two aspects of this task,
%% developing fast computation kernels and optimizing memory access.
%% The former corresponds the to the kernel component and
%% the latter is the target of overall strategies.

There are several approaches for obtaining optimized GEMM kernels,
%% (1) assembly programming, (2) auto-tuning, and (3) directive-based programming,
yielding different tradeoffs between performance and portability.
In GotoBLAS~\cite{gotoblas}, OpenBLAS~\cite{openblas} and BLIS~\cite{blis},
the kernel component of GEMM is written by domain experts in assembly.
ATLAS~\cite{atlas} adopts the auto-tuning method to
automatically generate kernels with different parameters
in C and find the best-performing one by running them
on the actual computing system.
POET~\cite{poet,poetcgo,poetmicro} and AUGEM~\cite{augem} use a 
directive-based programming approach.
Given a GEMM kernel in C, POET inserts annotations into
the C code to direct source-to-source compiler transformations.
AUGEM uses a template-based method to match
predefined patterns in the C code and transforms the matched
C code sequence into an optimized assembly code sequence.
Both the auto-tuning and directive-based programming approaches
rely on the compiler to transform kernels in C to machine instructions.
Kernel performance can be improved with compiler techniques,
including SIMD vectorization~\cite{Larsen:2000,Zhou:2016,Zhou:2016b,
  Eichenberger2004,GCCSLP2007},
polyhedral optimization~\cite{Bondhugula2008A,Kong:2013}, 
and loop tiling~\cite{Lam1991,Spampinato:2014,Xue00}.
Recently,
\textsc{Poca}~\cite{poca} leverages a wide range of architecture-specific
abstractions available in the LLVM compiler infrastructure
and proposes a series of domain-specific yet architecture-independent optimizations
to obtain high-performance kernels in a portable way.

The overall workload-partitioning strategy used in
GEMM is mainly concerned with choosing 
tiling factors, loop orders and parallelization
techniques.
The tiling factors $M_r$, $N_r$, $M_c$, $N_c$ and $K_c$
are essential to GEMM performance.
ATLAS~\cite{atlas} relies on auto-tuning to determine optimal
values for these factors.
Analytic techniques~\cite{analytic1,analytic2,blisanalytic} 
can also be used instead of auto-tuning.
These analytic methods generally take into consideration
the cache capacity and associativity
but assume an LRU replacement policy.
As far as we know, this paper is the first to
address the problem of cache sharing with non-LRU replacement policies,
and SCP serves as the first work to solve this problem.
In \cite{gotogemm}, a detailed discussion on
choosing different loop orders is given.
GEMM are usually parallelized only at layer 3 (the $ii$ loop),
but all loops along the $M$ and $N$ matrix dimensions
are potentially parallelizable.
BLIS~\cite{blispar} allows developers to specify a sophisticated configuration
so that more than one loops at layers 1, 3, 4 and 5
can be simultaneously parallelized.
This nested parallelization is well suited to complex architecture features
like multi-sockets and hyperthreading. 

As GEMM is highly optimized and parallelized,
other higher-level dense linear algebra operations,
such as Cholesky, LU and QR factorization, can
enjoy its performance benefits by building themselves
on top of GEMM.
In other words, this programming pattern in dense linear algebra software
harness a fork-and-join style parallelism.
To reduce synchronization overhead,
the PLASMA project~\cite{plasma2009,plasma2010,plasma2017}
makes use of a task-driven parallelization paradigm,
in which dense linear algorithms are formulated in
terms of 
directed-acyclic-graphs (DAGs) of fine-grained tasks.
The tasks are dynamically scheduled based on their
data dependencies (represented by the graph edges).
The basic idea behind SCP also applies to such
DAG-based parallelization.
Input matrices of different tasks can be packed into
different sets of a shared cache. As a result, 
the tasks can be scheduled based on the cache sets 
where the input matrices reside in, so that
the tasks whose input matrices reside 
in the same cache set
are scheduled to the same core.

\textcolor{blue}{
SCP is a cache partitioning technique.
While SCP is specific to GEMM running on architectures
with non-LRU caches,
there are general cache partitioning techniques \cite{cp0, cp1, cp2}
that focus on estimating the interference across 
the multiple applications
sharing the same cache and improving the
overall system performance.
In these research efforts, either hardware modifications are required
or some sophisticated cache replacement policies are 
needed.
In contrast,
SCP is a pure software approach that works for 
commercial processors.
}
