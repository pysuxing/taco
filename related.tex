\section{Related Work}\label{sec:related}
The idea that all level-3 BLAS operations can be built 
on top of 
a high-performance GEMM implementation was first proposed
in~\cite{gemmbased1,gemmbased2}.
The GotoBLAS library~\cite{gotoblas} and its successor OpenBLAS~\cite{openblas},
are instantiated based on this insight.
So optimizing GEMM has always been the central task in developing
dense linear algebra software.
The GEMM optimization has two aspects.
One is developing fast computation kernels and
the other is choosing a proper overall workload-partitioning  strategy
to optimize memory access.

%% Broadly speaking, there are two aspects of this task,
%% developing fast computation kernels and optimizing memory access.
%% The former corresponds the to the kernel component and
%% the latter is the target of overall strategies.

There are several approaches for obtaining optimized GEMM kernels,
%% (1) assembly programming, (2) auto-tuning, and (3) directive-based programming,
yielding different tradeoffs between performance and portability.
In GotoBLAS~\cite{gotoblas}, OpenBLAS~\cite{openblas} and BLIS~\cite{blis},
the kernel component of GEMM is written by domain experts in assembly.
ATLAS~\cite{atlas} adopts the auto-tuning method to
automatically generate kernels with different parameters
in C and find the best-performing one by running them
on the actual computing system.
POET~\cite{poet,poetcgo,poetmicro} and AUGEM~\cite{augem} use a 
directive-based programming approach.
Given a GEMM kernel in C, POET inserts annotations into
the C code to direct source-to-source compiler transformations.
AUGEM uses a template-based method to match
predefined patterns in the C code and transforms the matched
C code sequence into an optimized assembly code sequence.
Both the auto-tuning and directive-based programming approaches
rely on the compiler to transform kernels in C to machine instructions.
Kernel performance can be improved with compiler techniques,
including SIMD vectorization~\cite{Larsen:2000,Zhou:2016,Zhou:2016b,
  Eichenberger2004,GCCSLP2007},
polyhedral optimization~\cite{Bondhugula2008A,Kong:2013}, 
and loop tiling~\cite{Lam1991,Spampinato:2014,Xue00}.
Recently,
\textsc{Poca}~\cite{poca} leverages a wide range of architecture-specific
abstractions available in the LLVM compiler infrastructure
and proposes a series of domain-specific yet architecture-independent optimizations
to obtain high-performance kernels in a portable way.

The overall workload-partitioning strategy used in
GEMM is mainly concerned with choosing 
tiling factors, loop orders and parallelization
techniques.
The tiling factors $M_r$, $N_r$, $M_c$, $N_c$ and $K_c$
are essential to GEMM performance.
ATLAS~\cite{atlas} relies on auto-tuning to determine optimal
values for these factors.
Analytic techniques~\cite{analytic1,analytic2,blisanalytic} 
can also be used instead of auto-tuning.
These analytic methods generally take into consideration
the cache capacity and associativity
but assume an LRU replacement policy.
As far as we know, this paper is the first to
address the problem of cache sharing with non-LRU replacement policies,
and SCP serves as the first work to solve this problem.
In \cite{gotogemm}, a detailed discussion on
choosing different loop orders is given.
GEMM are usually parallelized only at layer 3 (the $ii$ loop),
but all loops along the $M$ and $N$ matrix dimensions
are potentially parallelizable.
BLIS~\cite{blispar} allows developers to specify a sophisticated configuration
so that more than one loops at layers 1, 3, 4 and 5
can be simultaneously parallelized.
This nested parallelization is well suited to complex architecture features
like multi-sockets and hyperthreading. 

