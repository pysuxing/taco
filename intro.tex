\section{Introduction}\label{sec:intro}
Dense linear algebra libraries are the most fundamental software in
scientific and engineering computing domains.
Basic Linear Algebra Subprograms (BLAS)~\cite{blas}  defines a collection
of APIs which act as standard building blocks for dense matrix operations.
BLAS routines are divided into 3 levels,
level-1 for vector-vector operations,
level-2 for matrix-vector operations,
and level-3 for matrix-matrix operations.
Processor vendors often provide BLAS implementations
that are highly optimized for their processors,
such as Intel MKL, AMD ACML and NVIDIA cuBLAS.
The HPC community has also contributed several high-quality
BLAS implementations, e.g., ATLAS~\cite{atlas},
GotoBLAS~\cite{gotoblas}, OpenBLAS~\cite{openblas},
and BLIS~\cite{blis,blisport}.

Among the three levels of BLAS routines, level-3 provides the most opportunities
for optimization because it has the highest computational complexity (of $O(n^3)$).
Among all the level-3 operations, GEneral Matrix Multiply (GEMM) is
of the most interest because the 
other level-3 operations can be defined
in terms of GEMM and some level-1 and level-2 operations~\cite{gemmbased1}.
In the past, much effort has been spent on optimizing GEMM for different
architectures~\cite{Liu2012,Wang2015,Volkov:2008,Cui11,blispar}.
%% both in general algorithm and in architecture specific ways.

%% GEMM performs a matrix-multiply-accumulate operation.
An optimized GEMM implementation consists of two components,
(1) a highly optimized kernel routine to accomplish its
computation, and
(2) an overall strategy to partition the workload into small tasks
and schedule these tasks to be executed effectively on the target processor.
The kernel routine is a serial program performing matrix-multiply-accumulate
on matrices from a single task.
The overall strategy partitions the workload by tiling 
the underlying loop nest
and determines a task schedule by choosing a specific traversal order
in the loop nest's iteration space.
In a multi-threaded context, different tasks (loop tiles) are assigned to
different threads to parallelize the whole operation.
The main objective of GEMM optimization is to
maximize the floating-point
operation throughput, measured by FLoating-point Operations Per Second ($flops$).
For the serial kernel routine, the optimization applies
instruction scheduling to improve instruction throughput.
For the workload partitioning, the optimization 
reorganizes memory accesses to reduce long memory latencies for
the kernel.
In this paper, we focus on optimizing the memory accesses for GEMM.

Two techniques, software prefetching and data packing,
have been widely used in GEMM implementations to speed up memory accesses.
Software prefetch instructions are utilized
to load data into cache before they are used, in 
order to ensure that memory accesses will not incur
long latencies and floating-point instructions
can execute at peak throughput.
To better exploit the capability of on-chip caches,
matrices are blocked and then packed into continuous memory buffers
that can fit into the caches under consideration
before the kernel routine is called.
Because GEMM is a computational intensive operation whose
arithmetic complexity is $O(n^3)$ and memory complexity is $O(n^2)$,
the overhead caused by data packing is negligible when
the input matrices are large~\cite{gotogemm}.


However, software prefetching and data packing cannot be effectively
applied to architectures with non-LRU shared caches.
In traditional high-performance processors, e.g., the Intel Xeon series,
both L1 and L2 caches are private to processor cores
and LRU replacement policy is adopted.
Without considering the last level cache (LLC),
the threads running on different cores prefetch their data into
different caches, and private data of different threads
cannot cause conflict cache misses.
This is no longer the case if caches are shared by several processor cores.
In this case, a cache line prefetched by one thread
may be evicted before its lifetime ends because another thread prefetches
another cache line into the same cache set.
A simple-minded solution would be to reduce 
the size of a packed matrix
so that the data used by several threads can simultaneously reside in a shared cache.
Due to the nature of set-associative caches,
this solution cannot completely eliminate inter-thread cache conflicts,
especially if the shared cache has a non-LRU replacement policy.
Our evaluation shows that inter-thread cache conflicts
in non-LRU shared caches
can heavily hurt GEMM performance,
even with packed matrices reduced to fit into the cache.

Following the trend in recent decades, modern architecture design
is introducing more and more cores on a single processor chip.
To reduce cost of on-chip cache memories and coherence networks,
shared caches may become common in future many-core processors.
In addition, non-LRU replacement policies, for example, pseudo-random,
may also be used to further reduce the cache design complexity.
Indeed, some high-performance processors based on the
ARM technology, e.g., the Phytium processor series~\cite{phytium},
%% I'm sure they have shared L2 caches,
%% but have not found any technical specification about the replacement policy
%% XGene from AppliedMicro~\cite{xgene},
%% ThunderX from Cavium~\cite{thunderx}
have already adopted this design. 
As a result, developing a better solution to reduce
the inter-thread cache conflicts 
for non-LRU shared caches is important.

In this paper, we present a Shared Cache Partitioning (SCP) method
to reduce inter-thread cache conflicts on architectures with non-LRU share caches.
The key idea is to partition a
share cache into physically disjoint sets
and assign different sets to different threads. This can be achieved by
exploiting the memory mapping mechanism of set-associative caches.

To the best of our knowledge, SCP represents
the first work addressing
the inter-thread cache conflicts problem on
architectures with non-LRU share caches.

The main contributions of this paper are as follows:
\begin{itemize}
\item We present a quantitative analysis of the negative effect of inter-thread cache
  conflicts on GEMM performance.
\item We propose SCP, a method for solving the inter-thread cache conflicts
  problem on architectures with non-LRU shared caches.
\item We have implemented SCP in the OpenBLAS library and evaluated it on Phytium 2000+,
  an emerging high-performance processor based on ARM's AArch64 architecture.
  Phytium 2000+ has 64 cores, private LRU L1 caches
  and pseudo-random shared L2 caches.
  Our evaluation shows that SCP can improve the GEMM performance consistently
  under various parallelism configurations, by $2.75\%$--$6.91\%$.
\end{itemize}


The rest of the paper is organized as follows.
Section~\ref{sec:background} gives some background on GEMM and discusses
the cache conflicts incurred.
Section~\ref{sec:scp} introduces the SCP method.
Section~\ref{sec:evaluation} presents and
analyzes our experimental results.
Section~\ref{sec:related} reviews the related work.
Finally, Section~\ref{sec:conclusion} concludes.
