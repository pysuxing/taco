\section{Performance Evaluation}\label{sec:evaluation}

We have implemented SCP in the OpenBLAS~\cite{openblas} library
%% (version 0.3.0-dev)
and evaluated it on a Phytium 2000+ processor.
The Phytium 2000+ processor is an emerging high-performance
64-core processor based on ARM's AArch64 architecture.
The 64 cores are organized into 16 clusters with each
cluster containing four cores.
The structure of a cluster is almost the same as
the four-core processor in Figure~\ref{fig:hierarchy}
except that the L2 caches of all 16 clusters connect to the main memory
with hardware coherence.
Table~\ref{tab:cluster} lists the
architectural parameters for the cores and caches
in this processor
and Table~\ref{tab:software} shows the software environment used.
We restrict our evaluation to DGEMM,
as in prior work~\cite{blispar,augem,poetmicro}, for two reasons.
First, the basic idea behind SCP applies to other
variants of GEMM such as SGEMM, CGEMM and ZGEMM.
Second, the LINPACK benchmark, which is used to build the
TOP500 list of world's most powerful supercomputers~\cite{top500},
relies on the DGEMM variant.

\begin{table}
  \centering
  \caption{Software environment used in evaluation}
  \label{tab:software}
  \begin{tabular}{lll}
    \toprule
    Environment & Software & Version \\
    \midrule
    Operating System & GNU/Linux & 4.4.0 AArch64 \\
    Compiler & GNU/GCC & 6.3.0 \\
    Thread Model & OpenMP & 4.0 \\
    BLAS & OpenBLAS & 0.3.0-dev \\
    \bottomrule
  \end{tabular}
\end{table}


In our evaluation, we use the tiling factors
computed as in Section~\ref{subsec:example}:
$M_r = 4$, $N_r = 8$, $K_c = 256$, $M_c = 192$, $N_c = 1024NT$,
as this combination causes GEMM to deliver the best performance 
when SCP is not used.
The degree of parallelism is controlled by two parameters,
the number of active clusters $NC$
and the number of active threads per cluster $NT_C$.
The total number of threads is $NT = NC \times NT_C$.
For the Phytium 2000+ processor, we have
$NC \in [1, 16]$ and $NT_C \in [1, 4]$. 
Generally, the DGEMM performance is measured in $flops$.
As $NT$ varies in our evaluation,
we use another metric derived from $flops$,
the average thread efficiency,
to describe our performance results consistently.
The average thread efficiency is a normalized value computed as
$E_{avg} = flops / (NT \times \widehat{flops})$,
where $\widehat{flops}$ stands for the theoretical performance
peak of a single core.
%% \begin{equation*}
%%   E = flops / (NT \cdot \widehat{flops})
%% \end{equation*}

We show how cache sharing can hurt GEMM performance
(Section~\ref{subsec:drawback}),
demonstrate how SCP can reduce
the penalty caused by cache sharing 
(Section~\ref{subsec:benefit}), conduct a 
quantitative analysis of the cache miss rates
(Section~\ref{subsec:analysis}), and
discuss the privatization of the shared matrix $B_2$
(Section~\ref{subsec:privb}).
%% Three versions of DGEMM are evaluated:
%% (1) Base, the baseline version,
%% (2) SCP,  the SCP version, 
%% and (3) SCP-P, the SCP version plus privatization of shared matrix $B_2$.

\subsection{Penalty of Cache Sharing}\label{subsec:drawback}

In a cluster, the L2 cache is shared by its four cores.
The impact of cache sharing can be demonstrated
by comparing the DGEMM performance results achieved
with the same $NT$
but different $NT_C$'s.
Figure~\ref{fig:drawback} shows the results.
Each curve represents a parallelization configuration.
Different subfigures are configured with different $NT$'s,
and different curves in the same subfigure are
configured with different $NT_C$'s.
As there is only one configuration for $NT=64$,
($NC=16$ and $NT_C=4$),
no comparison is available. Thus,
the results for $NT=64$ and $NT=32$ are combined in Figure~\ref{fig:drawback-32}.

\begin{figure}
  \centering
  \subfigure[$NT=4$]{
    \label{fig:drawback-4}
    \includegraphics[width=.45\textwidth]{figures/sec1-4}
  }
  \subfigure[$NT=8$]{
    \label{fig:drawback-8}
    \includegraphics[width=.45\textwidth]{figures/sec1-8}
  }
  \\
  \subfigure[$NT=16$]{
    \label{fig:drawback-16}
    \includegraphics[width=.45\textwidth]{figures/sec1-16}
  }
  \subfigure[$NT=32,64$]{
    \label{fig:drawback-32}
    \includegraphics[width=.45\textwidth]{figures/sec1-32}
  }
  \caption{Average thread efficiency with cache sharing}
  \label{fig:drawback}
\end{figure}

\begin{table}
  \centering
  \caption{Average thread efficiency with cache sharing}
  \label{tab:drawback}
  \begin{tabular}{cccccc}
    \toprule
     & $NT=4$ & $NT=8$ & $NT=16$ & $NT=32$ & $NT=64$ \\
    \midrule
    $NT_C=1$ & 91.46 & 91.30 & 90.30 & -     & - \\   
    $NT_C=2$ & 90.12 & 90.25 & 89.49 & 88.26 & - \\
    $NT_C=4$ & 84.05 & 80.61 & 79.74 & 79.81 & 71.71 \\
    %%  & $NT\!\!=\!\!4$ & $NT\!\!=\!\!8$ & $NT\!\!=\!\!16$ & $NT\!\!=\!\!32$ & $NT\!\!=\!\!64$ \\
    %% \midrule
    %% $NT_C\!\!=\!\!1$ & 91.46 & 91.30 & 90.30 & -     & - \\   
    %% $NT_C\!\!=\!\!2$ & 90.12 & 90.25 & 89.49 & 88.26 & - \\
    %% $NT_C\!\!=\!\!4$ & 84.05 & 80.61 & 79.74 & 79.81 & 71.71 \\
    \bottomrule
  \end{tabular}
\end{table}

For all $NT$'s, $NT_C=1$ achieves the 
best performance, followed by 
$NT_C=2$, and $NT_C=4$ comes at the lowest efficiency.
Table~\ref{tab:drawback} summarizes the average value
of $E_{avg}$ across all the matrix sizes.
Vertically, the results in the same column show how $E_{avg}$ varies with $NT_C$.
For all $NT$'s, $NT_C=1$ outperforms $NT_C=4$ by a large margin.
The performance gap is $7.4\%$ when $NT=4$ and roughly $10\%$ when $NT>4$.
Horizontally,  the results in the same row reflect the scalability of
DGEMM with certain $NT_C$'s.
With $NT_C=1$, the performance scales linearly with the number of threads.
With $NT_C=2$, $E_{avg}$ suffers a $1.86\%$ loss when $NT$ grows from 4 to 32.
The situation is much worse for $NT_C=4$, in which
$E_{avg}$ drops by $14.19\%$ from
$NT=4$ to $NT=64$.

This shows clearly that cache sharing has a considerable
impact on the DGEMM performance. 

\subsection{Effectiveness of SCP}\label{subsec:benefit}

\begin{figure}
  \centering
  \subfigure[$NT=4$]{
    \label{fig:benefit-4}
    \includegraphics[width=.45\textwidth]{figures/sec2-4}
  }
  \subfigure[$NT=8$]{
    \label{fig:benefit-8}
    \includegraphics[width=.45\textwidth]{figures/sec2-8}
  }
  \subfigure[$NT=16$]{
    \label{fig:benefit-16}
    \includegraphics[width=.45\textwidth]{figures/sec2-16}
  }
  \\
  \subfigure[$NT=32$]{
    \label{fig:benefit-32}
    \includegraphics[width=.45\textwidth]{figures/sec2-32}
  }
  \subfigure[$NT=64$]{
    \label{fig:benefit-64}
    \includegraphics[width=.45\textwidth]{figures/sec2-64}
  }
  \caption{Average thread efficiency under SCP}
  \label{fig:benefit}
\end{figure}

We evaluate SCP using the same method as in Section~\ref{subsec:drawback}.
SCP is not evaluated with $NT_C=1$ because there is no need
to partition the cache if $NT_C=1$.
Figure.~\ref{fig:benefit} shows the results.
For comparison purposes, the results without SCP
(denoted as Base) are also presented.

\begin{table}
  \centering
  \caption{Average thread efficiency improvement under SCP}
  \label{tab:win}
  \setlength{\tabcolsep}{3.5pt}
  \begin{tabular}{cccccc}
    \toprule
     & $NT=4$ & $NT=8$ & $NT=16$ & $NT=32$ & $NT=64$ \\
    \midrule
    $NT_C=2$ & 1.77 & 1.69 & 1.82 & 1.78 & - \\
    $NT_C=4$ & 2.75 & 4.62 & 4.41 & 3.11 & 6.91 \\
    \bottomrule
  \end{tabular}
\end{table}

%% \begin{table}
%%   \centering
%%   \caption{Average thread efficiency ($\%$) of SCP}
%%   \label{tab:benefit}
%%   \setlength{\tabcolsep}{3.5pt}
%%   \begin{tabular}{cccccc}
%%     \toprule
%%      & $NT\!\!=\!\!4$ & $NT\!\!=\!\!8$ & $NT\!\!=\!\!16$ & $NT\!\!=\!\!32$ & $NT\!\!=\!\!64$ \\
%%     \midrule
%%     $NT_C\!\!=\!\!2$ & 91.89/1.77 & 91.94/1.69 & 91.31/1.82 & 90.05/1.78 & - \\
%%     $NT_C\!\!=\!\!4$ & 86.80/2.75 & 85.22/4.62 & 84.15/4.41 & 82.92/3.11 & 78.62/6.91 \\
%%     \bottomrule
%%   \end{tabular}
%% \end{table}

Under all the parallelization configurations, 
SCP performs better than Base
consistently across all the matrix sizes evaluated.
Table~\ref{tab:win} summarizes the performance improvement of SCP over Base.
%% Table~\ref{tab:benefit} summarizes the average value
%% of $E_{avg}$ in SCP evaluation.
%% There are two values in each cell, separated by a slash.
%% $E_{avg}$ stands on the left,
%% and performance win over conventional packing
%% in the corresponding parallelism configuration
%% is shown on the right for convenience.
With $NT_C=2$, SCP performs slightly better than Base
by $1.7\%$ -- $1.8\%$.
The performance gap becomes larger with $NT_C=4$,
ranging from $2.75\%$ to $6.91\%$.
The largest performance gain, $6.91\%$, which is a considerable improvement,
is observed under the maximum parallelism with $NT=64$.
%% Looking at Table~\ref{tab:benefit} vertically,
%% we can see that SCP still cannot fully eliminate the impact
%% of cache sharing.
%% But it do relieve the situation to a certain degree.



\subsection{Cache Miss Rate Analysis}\label{subsec:analysis}

\begin{figure}
  \centering
  \subfigure[$NT=4$]{
    \label{fig:papi-4}
    \includegraphics[width=.45\textwidth]{figures/papi-4}
  }
  \subfigure[$NT=8$]{
    \label{fig:papi-8}
    \includegraphics[width=.45\textwidth]{figures/papi-8}
  }
  \subfigure[$NT=16$]{
    \label{fig:papi-16}
    \includegraphics[width=.45\textwidth]{figures/papi-16}
  }
  \\
  \subfigure[$NT=32$]{
    \label{fig:papi-32}
    \includegraphics[width=.45\textwidth]{figures/papi-32}
  }
  \subfigure[$NT=64$]{
    \label{fig:papi-64}
    \includegraphics[width=.45\textwidth]{figures/papi-64}
  }
  \caption{Cache miss rates with $NT_C=4$}
  \label{fig:papi}
\end{figure}

To understand why SCP outperforms Base and
why SCP is effective in reducing
inter-thread cache conflicts, 
we use the PAPI profiling tool~\cite{papi} to
analyze the cache behavior of GEMM. A total of 
four cache-related hardware events are counted,
which are listed and briefly described in Table~\ref{tab:events}.
Based on these hardware events, the
miss rates of L1 and L2 caches can be estimated  as 
$\frac{L1M}{L1M+L1H}$ and $\frac{L2M}{L2M+L2H}$, respectively.
In this analysis,
the hardware events are measured in a per-thread manner
and all the results are the average measurements
over all the threads.

\begin{table}
  \centering
  \caption{Hardware events in GEMM}
  \label{tab:events}
  \begin{tabular}{lll}
    \toprule
    Name & Event & Description \\
    \midrule
    L1M & PAPI\_L1\_DCM & L1 data cache misses \\
    L1H & PAPI\_L1\_DCH & L1 data cache hits \\
    L2M & PAPI\_L2\_DCM & L2 data cache misses \\
    L2H & PAPI\_L2\_DCH & L2 data cache hits \\
    \bottomrule
  \end{tabular}
\end{table}

Figure~\ref{fig:papi} shows the miss rates of L1 and L2 data caches.
We only give the results with $NT_C=4$
as SCP achieves the largest improvement with full cache sharing.
Table~\ref{tab:papi} summarizes their
average reductions across the matrix sizes used.
Under all the parallelization configurations,
both L1 and L2 cache miss rates are reduced under SCP, 
by roughly $13\%$ for L1 and nearly $10\%$ for L2.

For the Phytium 2000+ processor whose L1 caches are private,
only L2 caches are specially handled by SCP.
However,
the miss rates for both L1 and L2 caches
are reduced effectively by SCP.
While a reduction on the L2 miss rate is expected,
why is the L1 miss rate also reduced? In this processor,
every L2 cache is inclusive.
The conflict misses for the L2 cache not only cause
the data to be evicted from L2,
but also invalidate the evicted data's copy in all its
upper level L1 caches,
resulting in more L1 cache misses.
In general, the conflict misses for
an inclusive cache can affect
the performance of all its upper level caches.
As a result, improving the miss rate of a shared L2 cache
also improves the miss rates of all its upper level L1 caches.

Our results show that SCP can effectively
reduce the conflict misses in shared L2 caches.
In addition, the performance benefit thus obtained
also propagates to their upper level caches, too.

\begin{table}
  \centering
  \caption{Reduction of cache miss rates with $NT_C=4$}
  \label{tab:papi}
  \setlength{\tabcolsep}{3.5pt}
  \begin{tabular}{lccccc}
    \toprule
     & $NT=4$ & $NT=8$ & $NT=16$ & $NT=32$ & $NT=64$ \\
    \midrule
    L1     & 13.87 & 13.75 & 13.54 & 13.84 & 14.21 \\
    L2     & 10.00 & 10.00 & 8.27 & 8.56 & 7.62 \\
    \bottomrule
  \end{tabular}
\end{table}
%% \begin{table}
%%   \centering
%%   \caption{Cache miss rates ($\%$) with $NT_C=4$}
%%   \label{tab:papi}
%%   \setlength{\tabcolsep}{3.5pt}
%%   \begin{tabular}{lccccc}
%%     \toprule
%%      & $NT=4$ & $NT=8$ & $NT=16$ & $NT=32$ & $NT=64$ \\
%%     \midrule
%%     L1     & 7.81 & 7.83 & 7.87 & 7.88 & 7.88 \\
%%     L1 SCP & 6.73 & 6.75 & 6.81 & 6.81 & 6.81 \\
%%     L2     & 8.78 & 8.31 & 7.13 & 7.16 & 7.10 \\
%%     L2 SCP & 7.90 & 7.49 & 6.54 & 6.55 & 6.56 \\
%%     \bottomrule
%%   \end{tabular}
%% \end{table}

\subsection{Privatization of Shared Matrix}\label{subsec:privb}
In the previous evaluations, SCP only set-partitions the shared caches
for $A_2$ matrices, which are private to threads.
The shared $B_2$ matrix is still packed
in the conventional way-partitioning style.
As mentioned in Section~\ref{subsec:example},
set-partitioning can also be applied to $B_2$ after $B_2$ is privatized.
In this section, we discuss and evaluate this privatization alternative.

\begin{figure}
  \centering
  \subfigure[Conventional]{
    \label{fig:packb.conventional}
    \includegraphics[width=.65\textwidth]{figures/strategy-conventional}
  }
  \subfigure[Full privatization]{
    \label{fig:packb.full}
    \includegraphics[width=.65\textwidth]{figures/strategy-full}
  }
  \subfigure[Partial privatization]{
    \label{fig:packb.partial}
    \includegraphics[width=.65\textwidth]{figures/strategy-partial}
  }
  \caption{Packing workload ($B_2$) of thread $T_1$ with $NT_C=4$ and $NC=4$}
  %% \caption{Packing strategies of shared matrix $B_2$ with $NT_C=4$ and $NC=4$}
  \label{fig:packb}
\end{figure}

Privatization incurs more packing overhead because
the packing of $B_2$ must be done redundantly.
Figure~\ref{fig:packb} shows three packing strategies for $B_2$
in a parallelization configuration specified by
$NT_C=4$ and $NC=4$.
In each case, the entire packing workload of $B_2$ is divided into 16 tasks.
So only the packing workload of one thread, $T_1$, 
which is highlighted by the z-curves (as shown), is illustrated.
%% and the task assignment is annotated in Figure~\ref{fig:packb}.
In the conventional packing approach
(Figure~\ref{fig:packb.conventional}),
each thread takes a single task.
In full privatization
(Figure~\ref{fig:packb.full}),
all tasks are replicated to all
the threads.
As a result, the packing overhead grows in proportion to $NT$, which is unacceptable
if GEMM is highly parallelized, e.g., when $NT=64$.
In partial privatization 
(Figure~\ref{fig:packb.partial}), which is 
an alternative to full privatization,
privatization only occurs inside
a cluster so that the tasks for one cluster need not to be
replicated in the other clusters.
In this case, the extra packing overhead
is bounded by $NT_C$.
For example, on Phytium 2000+, the
overhead of packing $B_2$
in partial privatization is limited to 4 times
of that in the conventional packing approach.

Figure~\ref{fig:privb} shows the results for partial
privatization with $NT_C=4$ and $NC=4$.
The results for the other 
configurations are similar.
For comparison purposes,
SCP with and without privatization,
denoted as SCP-P and SCP, respectively,
are both given in Figure~\ref{fig:privb}.
Figure~\ref{fig:privb.papi} shows the L1 and L2
cache miss rates, which are 
slightly lower with privatization (SCP-P) than without
(SCP). Figure~\ref{fig:privb.ate} shows the average thread efficiency.
Despite its lower L1 and L2 cache miss rates,
privatization (SCP-P) suffers a performance loss
(relative to SCP) due to 
the extra packing overhead introduced by privatization.
Figure~\ref{fig:privb.breakdown} compares the
three types of major overheads in percentage terms 
over the total execution time, including
(1) packing of $A_2$, (2) packing of $B_2$,
and (3) synchronization.
The solid and dashed horizontal lines represent
the average values of the total overheads
across the matrix sizes
under SCP and SCP-P, respectively.
While behaving similarly as SCP
in (1) and (3), SCP-P is worse than SCP in (2).
To understand this further,
Table~\ref{tab:breakdown} gives a breakdown of each's
execution time in terms of (1) -- (3) and the
computation time of GEBP. As
the overhead for packing $B_2$
is multiplied by a factor of $NT_C=4$,
the total overhead has thus increased
from $2.35\%$ (under SCP) to $4.72\%$ (under SCP-P).

\begin{figure}
  \centering
  \subfigure[Cache miss rates]{
    \label{fig:privb.papi}
    \includegraphics[width=.45\textwidth]{figures/privb-papi}
  }
  \subfigure[Average thread efficiency]{
    \label{fig:privb.ate}
    \includegraphics[width=.45\textwidth]{figures/privb-ate}
  }
  \subfigure[Execution time breakdown]{
    \label{fig:privb.breakdown}
    \includegraphics[width=.90\textwidth]{figures/privb-breakdown}
  }
  \caption{Performance of partial privatization with $NT_C=4$ and $NC=4$}
  \label{fig:privb}
\end{figure}

\begin{table}
  \centering
  \caption{Execution time breakdown with $NT_C=4$ and $NC=4$}
  \label{tab:breakdown}
  \setlength{\tabcolsep}{3.5pt}
  \begin{tabular}{lcccc}
    \toprule
     & Sync & PackA & PackB & GEBP\\
    \midrule
    SCP   & 1.01 & 0.53 & 0.80 & 97.65 \\
    SCP-P & 1.00 & 0.52 & 3.20 & 95.28\\
    \bottomrule
  \end{tabular}
\end{table}


Our results demonstrate that privatizing the shared matrix $B_2$ can further
reduce inter-thread cache conflicts.
However, the improvement in cache performance is small.
As $B_2$ only occupies a small portion of the shared L2 cache during GEMM execution,
this marginal improvement is not sufficient to
balance the extra packing overhead introduced
by privatization.
