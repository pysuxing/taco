\documentclass[]{article}

\usepackage{booktabs} % For formal tables
\usepackage{xcolor}

\begin{document}

\title{Author Response}
\date{}
\author{}



\maketitle

Thanks to all the reviewers for their comments.

\section{Response to Reviewer 1}

\begin{verbatim}
Although the explanation of the problem and the solution is very
clear in general, I think that it would be useful to include an
explanation of how the mapping from virtual to physical addresses
could create additional conflicts in the shared cache, and an analysis
of whether this could happen in the target system or not.
\end{verbatim}


Added. See the new sentences added in lines -2 -- -1 in
page 10 and lines 1 -- 3 in Page 11 (Section 4).

\section{Response to Reviewer 2}

\begin{verbatim}
Why are the selected matrix sizes (3072 to 4096-6144) chosen?
\end{verbatim}


Added. See the 2nd last paragraph added just before
Section 4.1.

\begin{verbatim}
Which is the behavior of the approach for smaller matrices?
\end{verbatim}

The performance of GEMM on small matrices may be
sub-optimal
and unstable.
It is common to omit small
matrices as GEMM is meant to run on large ones.
There is a variant of BLAS specially designed for
large amounts of small matrices,
Batched-BLAS, which is out of the scope of this paper.

\begin{verbatim}
Why is there a systematic drop for the largest tested matrices on
Figs. 5 and 6?
\end{verbatim}

There are two kinds of performance drops here:

(1) {\bf Performance drops as the matrix size increases 
under $NT \ge 16$.}
There is a synchronization point at the end of each iteration
of the loop at layer 2 (Fig.~2).
The synchronization is an all-to-all 
communication, whose overhead is proportional to $NT^2$.
As the matrix size increases, GEMM suffers
from more synchronization because
the loop at layer 2 runs for more iterations.
When $NT$ is 4 or 8, the overhead is small but
increases quickly as $NT$ increases.

(2) {\bf Performance drops at matrix sizes that are
multiples of 1024.}
After some  profiling, we found that at these sizes,
GEMM suffers from more memory contention.
As an example, the table below shows the profiling results
for matrix sizes 3968 and 4096
under $NT_C=4$ and $NC=4$.
$BW_{A}$ ($BW_{B}$) is the measured bandwidth for 
packing A (B), and
$E_{avg}$ is the floating-point efficiency.
We can see that there is a big performance gap
in $BW_{A}$ and $BW_{B}$.
$E_{avg}$ is also a little lower at 4096 than at 3840,
which may also be caused by the memory contention.
In this example, $NT=NT_C \times NC=16$.
When $M=N=K=4096$, each thread packs a $256 \times 256$ submatrix
from $A$ and a $256 \times 256$ submatrix from $B$,
both of size $V=512KB$.
When $M=N=K=3840$, the submatrix to be packed is 
of size $V=480KB$.
If the 16 threads start packing roughly at the same time,
they will access the following memory addresses:
$$addr+0V, addr+1V, \cdots, addr+15V$$
Due to the organization of the DDR memory hardware,
$V=512K$ is more likely to cause memory contention than $V=480KB$
because the memory banks are accessed in a non-balanced manner.

\begin{table}[h]
  \centering
  \caption{Profiling results with $NT_C=4$ and $NC=4$}
  \vspace{1em}
  \begin{tabular}{cccc}
    \toprule
         & $BW_A$   & $BW_B$   & $E_{avg}$\\
    \midrule
    3840 & 0.81GB/s & 0.44GB/s & 87.2\%   \\
    4096 & 1.44GB/s & 0.89GB/s & 86.1\%   \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{verbatim}
Is there any evidence that this methodology transparently applies to 
other BLAS-3 routines in OpenBLAS (it should be trivial to test) and
for all matrix layouts (transposed/non transposed)?
\end{verbatim}

As other level-3 routines, e.g., TRSM, TRMM, SYMM and HEMM,
call GEMM internally, they should also benefit from our work.
So the SCP methodology can apply transparently to 
these other BLAS-3 routines as well.

Internally, different matrix layouts (row-major/column-major)
are canonicalized to an implementation defined form
by optionally swapping the $A$ and $B$ operands in $C=\alpha A B+\beta C$.
For example, the row-major $C=\alpha A B+\beta C$ can be transformed to
the column-major $C^T=\alpha B^T A^T + \beta C^T$.
Then by means of data packing, the
matrix orientation information (transposed vs. non-transposed)
is eliminated and the matrix data are
packed into the special
in-memory representation used by the kernel routine, as demonstrated in Fig. 2.


SCP, which works after these transformations, 
does not rely on
specific matrix layouts or orientations.

\section{Response to Reviewer 3}

\begin{verbatim}
There is a typo on Page 7, line 43, turning -> tuning.
\end{verbatim}

Fixed.

\begin{verbatim}
As the authors mentioned the related work on the tuning implementation
of GEMM only, they should also review the work regarding cache 
partitioning because this paper discussion is mainly on cache
partitioning. Some important papers are: ...
\end{verbatim}

The recommended references are now cited and
discussed in the last paragraph of Section 5.

\begin{verbatim}
The proposed method works well, but it is so specific to DGEMM and
Phytium 2000+ processor. To confirm that their approach is much more
widely applicable even in the context of DGEMM, the discussion when
using the different types of processors with a cache hierarchy for
multi-core environments such as Xeon processors and KNL is strongly
required.
\end{verbatim}

For Xeon and Xeon Phi processors, the caches with
the LRU replacement policy are used.
However,
SCP is designed for processors with non-LRU caches.

SCP works for all other BLAS-3 routines, e.g., TRSM, TRMM, SYMM and HEMM, that call GEMM internally.
SCP is not specific to the Phytium 2000+ processor and
should work for any processors with shared non-LRU caches.


\end{document}
