\section{SCP: The Shared Cache Partitioning Method}\label{sec:scp}

This section introduces our SCP method for reducing
inter-thread cache conflicts.
The key insight is that inter-thread cache conflicts can be eliminated
if packed matrices used by different threads are fetched to different cache sets in a shared L2 cache.
In Section~\ref{subsec:example}, we illustrate 
SCP by an example.  In Section~\ref{subsec:formal},
we give the algorithms in our SCP method.

\subsection{An Example}\label{subsec:example}

Suppose we want to implement a double precision GEMM (DGEMM) on
the 4-core processor, where $es=8B$.
We now determine the tiling factors used based on
the constraints (\ref{eq:constraints.reg}) -- (\ref{eq:constraints.l3}).
Many solutions for ($M_r$, $N_r$) can satisfy (\ref{eq:constraints.reg}), including
$(4,8)$, $(8,4)$, $(6,8)$, and $(8,6)$. Auto
tuning returns
$M_r = 4$ and $N_r = 8$ as the best.
By (\ref{eq:constraints.l1}),
$K_c \le \lfloor c_{L1}/es/nt_{L1}/(N_r + 2 M_r) \rfloor = \lfloor 32KB/8B/1/16 \rfloor = 256$.
Here, we choose $K_c=256$.
By (\ref{eq:constraints.l2}),
$M_c \le \lfloor (c_{L2}/es/nt_{L2} - 2 N_r K_c )/ K_c \rfloor =
\lfloor (2MB/8B/4 - 2*8*256)/256 \rfloor = 240$.
To leave space for other data structures and the program code,
it is reasonable to shrink $M_c$ slightly, with,
for example, $192$, $208$ and $224$ as good
candidates. Again, a turning process yields
$M_c = 192$.
As there is no L3 cache, (\ref{eq:constraints.l3}) can be ignored
and $N_c$ can be given a large value.
Here, we choose $N_c = 1024NT = 4096$.
With these tiling factors determined, the sizes of packed matrices
can be calculated. Table.~\ref{tab:msizes} lists
the sizes of various packed matrices used.

\begin{table}
  \centering
  \caption{Sizes of packed matrices in DGEMM ($es = 8B$)}
  \label{tab:msizes}
  \begin{tabular}{cl|cl}
    \toprule
    Matrix & Size & Matrix & Size \\
    \midrule
    %% $A_2$, $A_3$ & $es M_c K_c = 384KB$ & $B_2$ & $es N_c K_c = 8MB$ \\
    %% $A_4$ & $es M_r K_c = 8KB$   & $B_3$, $B_4$ & $es N_r K_c = 16KB$ \\
    $A_2$ & $es M_c K_c = 384KB$ & $B_2$ & $es N_c K_c = 8MB$ \\
    $A_3$ & $es M_c K_c = 384KB$ & $B_3$ & $es N_r K_c = 16KB$ \\
    $A_4$ & $es M_r K_c = 8KB$   & $B_4$ & $es N_r K_c = 16KB$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \centering
  \subfigure[Way-partitioning]{
    \label{fig:partition.conventional}
    \includegraphics[width=.4\textwidth]{figures/wpart}
  }
  \subfigure[Set-partitioning]{
    \label{fig:partition.segmented}
    \includegraphics[width=.4\textwidth]{figures/spart}
  }
  \caption{Partition styles of the shared L2 cache (n=16, ns=2048)}
  \label{fig:partition}
\end{figure}

Each $A_2$ is packed into a single continuous buffer.
Figure~\ref{fig:partition.conventional} shows how the
$A_2$ instances of 4 threads are distributed in the shared 16-way L2 cache.
Every cache set $s \in [0,2048)$ contains
data from all 4 $A_2$ instances,
potentially leading to inter-thread cache conflicts.
What if the $A_2$ matrices are distributed in the way shown in
Figure~\ref{fig:partition.segmented}?
$A_2$ of different threads live in strictly disjoint cache sets, enabling
the inter-thread cache conflicts caused by $A_2$ to be
eliminated completely!

Figures~\ref{fig:partition.conventional} and~\ref{fig:partition.segmented}
essentially represent two different partitioning styles for a shared cache,
referred to as \emph{way-partitioning} and \emph{set-partitioning}, respectively.
To obtain the data layout in set-partitioning,
$A_2$ can no longer be stored in a single continuous buffer.
Instead, $A_2$ will be distributed to 12 continuous memory segments, each of size $32KB$.
The distance from one segment to the next is equal to $wc_2=128KB$.
%% There is no memory fragment because the segment size is a multiple of
%% size of $A_4$ (8KB).
%% FIXME put this to the end of this section
While avoiding inter-thread cache conflicts,
set-partitioning
introduces some complexity to GEMM implementations
because the GEBP kernel and the matrix packing routines
now must work with segmented memory buffers instead of continuous ones.

What about $B_2$, the other packed matrix used in GEBP?
Unlike $A_2$, which is thread-private,
$B_2$ is shared among all threads and
the packing of $B_2$ is done collaboratively by all threads.
There are two choices (with different tradeoffs), (1) privatize $B_2$ and apply 
set-partitioning and (2) 
fall back to the conventional way-partitioning 
for $B_2$.
$B_2$ can be made thread-private if every thread makes
its own pack of the whole $B_1$ matrix.
The first choice achieves a full per-thread data isolation
at the expense of redundant packing overhead.
The second choice avoids the extra overhead in both time and space
but potentially increases the inter-thread conflicts caused by the shared $B_2$ matrix.
In this work,
these two choices are evaluated and compared in Section~\ref{sec:evaluation}.

\subsection{Algorithms}\label{subsec:formal}

Given the tiling factors $M_r$, $N_r$, $K_c$, $M_c$, $N_c$,
and architectural details of the memory hierarchy,
SCP will systematically determine the
the memory layout for the
packed matrices $A_2$ and $B_2$ used in GEMM.
The memory layout of the other packed matrices at
lower layers are determined by the memory layout of
$A_2$ and $B_2$.
We assume that the share matrix $B_2$ is packed
in the conventional way-partitioning style.
If desired, $B_2$ can be privatized and handled in the same way as $A_2$.

we make a few standard assumptions, which usually
hold for the architecture considered by SCP:
\begin{itemize}
\item The architecture has 2 or 3 levels of cache; %% i.e. $NL=2$ or $NL=3$
\item All caches are inclusive set-associative caches; %% i.e. $n_l \ge 2$ for $l \in [1, NL]$
\item Caches at the same level are homogeneous; and %% i.e. $L_l^i = L_l^j$ for $i \ne j$, $l \in [1, NL]$.
\item A cache's way-capacity is a multiple of the sum of way-capacities of its children,
i.e., $(\frac{nt_l}{nt_{l-1}} \cdot wc_{l-1}) \mid wc_l$.
\end{itemize}

We use a memory descriptor $\mathcal{D}_t^M$ to specify 
the memory space occupied by a 
packed matrix $M$ used by thread $t$.
The memory descriptor is essentially a subspace of
the whole address space, i.e., $\mathcal{D}_t^M \subset \mathcal{A}$.
%% The memory layout of a packed matrix is described by a memory descriptor,
%% which is essentially a subspace of the whole address space $\mathcal{A}$.
%% Each packed matrix $M$ used by thread $t$,
%% is associated with a memory descriptor $\mathcal{D}_t^M$
%% specifying the memory space it occupies.
Generally, $\mathcal{D}_t^M$ consists of a sequence of $N_t^M$ disjoint memory segments,
$\mathcal{D}_t^M = \{ S_0, S_1, \cdots, S_{N_t^M}\}$.
%% in which $S_i \bigcap S_j = \phi$ if $i \ne j$.
A segment is represented by its start and end addresses,
$S_i = [s_i, e_i)$,
in which $s_i < e_i \in \mathcal{A}$.
With way-partitioning, the memory descriptor contains only one segment.
With set-partitioning,
the memory descriptor contains a sequence of equally strided segments.

%% FIXME use l as the level iterator?
\begin{algorithm}
  \caption{SCP phase 1: allocate memory buffers}
  \label{alg:scp.phase1}
  \begin{algorithmic}[1]
    \REQUIRE $L_l = (c_l,l_l,n_l,nt_l)$ for $l \in [0, NL]$, $NT$,
    $M_r$, $N_r$, $K_c$, $M_c$, $N_c$
    \ENSURE $\mathcal{D}_p$ and $\mathcal{D}_s$
    \STATE $size_p \gets es M_c K_c$ \label{line:size.p}
    \STATE $size_s \gets es N_c K_c / NT$ \label{line:size.s}
    \STATE $align \gets l_1$ \label{line:align.init}
    \FOR {$l=1$ to $NL$} \label{line:align.for}
    \IF {$nt_l / nt_{l-1} > 1$ \AND $L_l$ is non-LRU} \label{line:align.type}
    \STATE $align \gets lcm(align, wc_l / nt_l)$ \label{line:align.update}
    \ENDIF
    \ENDFOR \label{line:align.endfor}
    \STATE $size_p \gets \lceil size_p / align \rceil \cdot align$ \label{line:align}
    %% \STATE $size \gets NT \cdot size_p + size_s$
    \STATE $addr_p \gets allocate(\mathcal{A}, size_p \cdot NT)$ \label{line:alloc.begin}
    \STATE $addr_s \gets allocate(\mathcal{A}, size_s \cdot NT)$ \label{line:alloc.end}
    \STATE $\mathcal{D}_p \gets \lbrace [addr_p, addr_p + size_p \cdot NT) \rbrace$ \label{line:d.begin}
    \STATE $\mathcal{D}_s \gets \lbrace [addr_s, addr_s + size_s \cdot NT) \rbrace$ \label{line:d.end}
  \end{algorithmic}
\end{algorithm}

SCP runs in three phases.
The first phase allocates memory buffers for all $A_2$ and $B_2$
instances used in GEMM. There are two buffers, $\mathcal{D}_p$ and $\mathcal{D}_s$,
for storing private and shared matrices, respectively.
The second phase computes memory descriptors for $A_2$ instances
by partitioning $\mathcal{D}_p$ into $NT$ thread-private buffers
$\mathcal{D}_t^{A_2}$ , where $t \in [0, NT)$.
Buffers of different threads are disjoint,
i.e., $\mathcal{D}_i^{A_2} \bigcap \mathcal{D}_j^{A_2} = \emptyset$ if $i \ne j$.
The third phase computes memory descriptors for $B_2$.
Because is $B_2$ is shared among all threads,
$\mathcal{D}_t^{B_2}$ does not represent the whole $B_2$ matrix,
but the part (of size $\frac{es N_c K_c}{NT}$) which is packed by thread $t$.
The work flow of these three phases is listed
in Algorithms~\ref{alg:scp.phase1} -- \ref{alg:scp.phase3}.

In Algorithm~\ref{alg:scp.phase1},
the per-thread sizes for memory buffers $\mathcal{D}_p$ and
$\mathcal{D}_s$ are computed
(lines~\ref{line:size.p} -- \ref{line:size.s}).
Because the memory space in $\mathcal{D}_p$ will be set-partitioned,
extra efforts are made to ensure a proper alignment for $\mathcal{D}_p$.
Then $align$ is found
(lines~\ref{line:align.init} -- \ref{line:align.endfor}) 
and $size_p$ is enlarged to a multiple of $align$ (line~\ref{line:align}).
Specifically,
$align$ is initialized with the L1 cache line size $l_1$.
Then for each level $l$ (line~\ref{line:align.for})
with shared non-LRU caches (line~\ref{line:align.type}),
$align$ is updated by computing the least-common-multiple of
the earlier $align$ and $wc_l/nt_l$ (line~\ref{line:align.update}).
The insight on using $wc_l/nt_l$ is that
each way of the level-$l$ cache should be
equally partitioned among the $nt_l$ threads
in set-partitioning.
Finally, memory is allocated (lines~\ref{line:alloc.begin} -- \ref{line:alloc.end})
and buffers $\mathcal{D}_p$ and $\mathcal{D}_s$
are created (line~\ref{line:d.begin} -- \ref{line:d.end}).

\begin{algorithm}
  %% a trick to temporally define customized command
  \renewcommand{\algorithmicprint}{\textbf{call}}
  \renewcommand{\algorithmicwhile}{\textbf{procedure}}
  \renewcommand{\algorithmicendwhile}{\textbf{end procedure}}
  \caption{SCP phase 2: compute descriptors for $A_2$}
  \label{alg:scp.phase2}
  \begin{algorithmic}[1]
    \REQUIRE $L_l = (c_l,l_l,n_l,nt_l)$ for $l \in [0, NL]$, $\mathcal{D}_p$, $NT$
    \ENSURE $\mathcal{D}_t^{A_2}$ for $t \in [0, NT)$
    %% \ENSURE $\mathcal{D}_l^i$ for $l \in [0, NL]$ and $i \in [0, NT/nt_l]$
    \STATE $\mathcal{D}_{NL+1}^0 \gets \mathcal{D}_p$ \label{line:memory.d}
    \STATE $nt_{NL+1} \gets NT$ \label{line:memory.nt}
    \PRINT $subspace(NL+1, 0, \mathcal{D}_{NL+1}^0)$ \label{line:subspace.root}
    \STATE                      % for a blank line
    \WHILE {$subspace\ (l, idx, \mathcal{D}_l^{idx})\ $} \label{line:subspace.begin}
    \IF {$l = 0$} \label{line:shortcut.begin}
    \STATE $\mathcal{D}_{idx}^{A_2} \gets \mathcal{D}_l^{idx}$
    \RETURN
    \ENDIF \label{line:shortcut.end}
    \STATE $nchildren \gets nt_l / nt_{l-1}$ \label{line:nchildren}
    \FOR {$i = 0$ to $nchildren-1$} \label{line:for.begin}
    \STATE $cidx \gets idx \cdot nchildren + i$ \label{line:cidx}
    \IF {$l = NL+1$ \OR $L_l$ is LRU} \label{line:if}
    \STATE $lb \gets min(\mathcal{D}_l^{idx})$
    \STATE $ub \gets max(\mathcal{D}_l^{idx})+1$
    \STATE $len \gets (ub - lb) / nchildren$
    \STATE $\mathcal{D}_{temp} \gets [lb + i \cdot len, lb + (i+1) \cdot len)$
    \ELSE \label{line:else}
    \STATE $len \gets ns_{l} / nchildren$
    \STATE $\mathcal{S}_i \gets [i \cdot len, (i+1) \cdot len)$
    \STATE $\mathcal{D}_{temp} \gets \varphi_l^{-1}(\mathcal{S}_i)$
    \ENDIF \label{line:endif}
    \STATE $\mathcal{D}_{l-1}^{cidx} \gets \mathcal{D}_{l}^{idx} \bigcap \mathcal{D}_{temp}$
    \label{line:childspace}
    \PRINT $subspace(l-1, cidx, \mathcal{D}_{l-1}^{cidx})$ \label{line:recursive}
    \ENDFOR \label{line:for.end}
    \ENDWHILE \label{line:subspace.end}
  \end{algorithmic}
\end{algorithm}

%% FIXME memory buffer? memory space? subspace?
Algorithm~\ref{alg:scp.phase2} describes
the second phase of SCP.
The main component is a recursive procedure $subspace$
(lines~\ref{line:subspace.begin} -- \ref{line:subspace.end}).
The $subspace$ procedure traverses the memory hierarchy
to compute for each node, including the processor cores and main memory,
a subspace of $\mathcal{D}_p$.
This procedure
has three input parameters, $l$ and $idx$ for
identifying the node on which it is running,
and a subspace $\mathcal{D}_l^{idx} \subset \mathcal{D}_p$ allocated to the node. 
The functionality of $subspace$ is to partition $\mathcal{D}_l^{idx}$
among all the children of node $(l, idx)$.
If $subspace$ encounters a level 0 node, i.e.,
a processor core,
then $\mathcal{D}_l^{idx}$ is exactly the thread-private
memory buffer $\mathcal{D}_{idx}$ for thread $idx$,
causing $subspace$ to return early
(lines~\ref{line:shortcut.begin} -- \ref{line:shortcut.end}).
Otherwise, the number of children is computed (line~\ref{line:nchildren})
and $subspace$ iterates over all the child nodes
(lines~\ref{line:for.begin} -- \ref{line:for.end}).
For each child node $cidx$ at layer $l-1$ (line~\ref{line:cidx}),
its memory space $\mathcal{D}_{l-1}^{cidx}$ is computed
and $subspace$ is called recursively on it (line~\ref{line:recursive}).
$\mathcal{D}_{l-1}^{cidx} \subset \mathcal{D}_l^{idx}$ is obtained
by intersecting $\mathcal{D}_l^{idx}$ with
a temporal memory space $\mathcal{D}_{temp}$ (line~\ref{line:childspace}).
The if-else branch (lines~\ref{line:if} -- \ref{line:endif})
represents two distinct ways for computing
$\mathcal{D}_{temp}$.
If the node is main memory or a LRU cache (line~\ref{line:if}),
$\mathcal{D}_l^{idx}$ is partitioned in the conventional
way-partitioning style.
Otherwise, the else branch (line~\ref{line:else}) performs
a set-partitioning on $\mathcal{D}_l^{idx}$.
Initially, the main memory gets the whole $\mathcal{D}_p$ (line~\ref{line:memory.d})
and $nt_{NL+1}$ is set to $NT$ (line~\ref{line:memory.nt})
because the main memory is shared by all threads.
Then $subspace$ procedure starts from the main memory (line~\ref{line:subspace.root})
and traverses the memory hierarchy in a depth-first-search order.

The computation of $\mathcal{D}_t^{B_2}$ is quite simple.
Algorithm~\ref{alg:scp.phase3} divides $\mathcal{D}_s$
into $NT$ partitions, one for each thread.
First, the range of $\mathcal{D}_s$ is obtained
(lines~\ref{line:lb} -- \ref{line:ub}) and
the capacity of $\mathcal{D}_s$ is equally
divided among the $NT$ threads (line~\ref{line:len}).
Then $\mathcal{D}_s$ is partitioned in a way-partitioning style
and each thread $t$ get buffer space for its part in $B_2$
(line~\ref{line:thread.for}--\ref{line:thread.forend}).

\begin{algorithm}
  \caption{SCP phase 3: compute descriptors for $B_2$}
  \label{alg:scp.phase3}
  \begin{algorithmic}[1]
    \REQUIRE $L_l = (c_l,l_l,n_l,nt_l)$ for $l \in [0, NL]$, $\mathcal{D}_s$
    \ENSURE $\mathcal{D}_t^{B_2}$ for $t \in [0, NT)$
    \STATE $lb \gets min(\mathcal{D}_s)$ \label{line:lb}
    \STATE $ub \gets max(\mathcal{D}_s)+1$ \label{line:ub}
    \STATE $len \gets (ub - lb) / nchildren$ \label{line:len}
    \FOR {$t=0$ to $NT-1$} \label{line:thread.for}
    \STATE $\mathcal{D}_t^{B_2} \gets \lbrace [lb + t \cdot len, lb + (t+1) \cdot len) \rbrace$
    \ENDFOR \label{line:thread.forend}
  \end{algorithmic}
\end{algorithm}

