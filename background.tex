\section{Background}\label{sec:background}

\subsection{The Memory Hierachy Review}\label{subsec:hierachy}

Modern architectures take advantage of fast on-chip caches
to overcome the ``memory wall''.
As a compromise between direct mapped cache and full-associative cache,
set-associative cache has been the dominant choice for real-world architectures.
To understand how shared cache can cause inter-thread cache conflicts
and do harm to GEMM performance, we review some details
about the working mechanisim of set-associative caches.

A set-associative cache can be defined by a 4-tuple $(c, l, n, nt)$,
in which $c$ is the cache capacity, $l$ is the cache line size,
$n$ is ways-of-associativity,
and $nt$ means the number of cores sharing the cache.
The whole cache memory is divided into $ns=c/l/n$ sets,
denoted by integral indices $\mathcal{S} = [0, ns)$,
with each set containing $n$ cache lines.
Generally, $ns$ is a power-of-two value.
An indexing function $\varphi: \mathcal{A} \mapsto \mathcal{S}$ is responsible for
mapping addresses from the address space $\mathcal{A}$
(either virtual or physical) to cache sets $\mathcal{S}$.
Data at $addr \in \mathcal{A}$ must be fetched
into the cache set $\varphi(addr) \in \mathcal{S}$ if requested.
A classic implementation of the indexing function is shown below,
%% $\varphi(addr) = (addr \gg log_2(l)) ~\&~ (ns-1)$,
with $\gg$ and $\&$ representing logical-shift-right and bitwise-and operations, respectively.
It is an extraction of the $log_2(ns)$ bits starting from the $log_2(l)$
position in $addr$'s binary form.
\begin{equation*}
  \varphi(addr) = (addr \gg log_2(l)) ~\&~ (ns-1)
  \label{eq:phi}
\end{equation*}

It is easy to prove that any continous memory block no larger than $c/n$
cannot conflict with itself.
We define a quantity, way-capacity, denoted as $wc=c/n$,
to represent this size bound for conflict-free memory blocks.

The main memory, $NL$ levels of caches, and $NT$ processor cores
form a memory hierachy generally organized into
a $NL+2$ level regular tree structure.
Processor cores at level $0$ are leaves,
caches at levels $1$ -- $NL$ are intermedia nodes
and the main memory at level $NL+1$ is the tree root.
Any node in the memory hierachy can be identified by
its layer and its unique index on the layer $(layer, index)$.
The number of nodes on layer $l$ is $NT / nt_l$.

Throughout this paper we will use a 4-core processor with
$NL=2$ levels of caches as the platform for SCP demonstration.
Figure.~\ref{fig:hierachy} shows its memory hierachy and
detailed architectural parameters are listed in Table.~\ref{tab:cluster}.
Each core has 32 128-bit (16B) vector registers,
each capable of store 2 double precision floating-point numbers.
To make a consistency, the processor cores on level 0
is represented by its registers, which can be viewed as a special
level-0 cache $L0 = (512B, 16B, 32, 1)$ with programmable replacement policy.

\begin{figure}
  \centering
  \includegraphics[width=.45\textwidth]{figures/cluster-new}
  \caption{Memory hierachy of the 4-core processor
    (nodes labeled with $(layer,index)$)}
  \label{fig:hierachy}
\end{figure}

\begin{table}
  \centering
  \caption{Architectural parameters of the 4-core processor}
  %% (see Section~\ref{subsec:hierachy} for meanings of symbols in the header row)
  \label{tab:cluster}
  \begin{tabular}{lccccccl}
    \toprule
    type & $c$ & $n$ & $l$ & $nt$ & $ns$ & $wc$ & policy \\
    \midrule
    L0 Registers  & 512B & 32 & 16B & 1 & 1 & 16B & programmable \\
    L1 Data    & 32KB & 2  & 64B & 1 & 256 & 16KB & LRU \\
    L2 Unified & 2MB  & 16 & 64B & 4 & 2048 & 128KB & psudo-random \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Structure of GEMM}\label{subsec:gemm}

GEMM performs a matrix-multiply-accumulate operation $C = \beta C + \alpha A B$,
where $A, B$ and $C$ are matrices of sizes
$M \times K$, $K \times N$ and $M \times N$, respectively,
and $\alpha$, $\beta$ are scalars.
While this operation is algorithmically simple,
so that a 3-deep loop nest suffices to accomplish it,
a high-performance implementation can be quite
sophisticated due to the presence of multi-level memory
hierarchies on modern processors.
Figure.~\ref{fig:gemm} shows the structure of GEMM program from
the OpenBLAS library~\cite{openblas}.
Each loop (in the original 3-deep loop nest) is tiled,
resulting in a total of six loops (referred to as layers 1 -- 6).
%% Loop tiling, together with data packing and prefetching,
%% serves to improve data locality and overlap computation
%% and memory access effectively.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/gemm}
  \caption{Structure of blocked GEMM}
  \label{fig:gemm}
\end{figure*}

In this blocked algorithm, the loops over the $N$, $K$ and $M$
matrix dimensions are tiled by sizes $N_c$, $K_c$ and $M_c$ at 
layers 1, 2 and 3, respectively.
At layers 4 and 5, the $N$ and $M$ dimensions are further tiled by sizes
$N_r$ and $M_r$, respectively.
As a result, the innermost loop at layer 6 goes over the $K$
dimension for a total of $K_c$ times,
with each iteration performing a rank-1 update on
the $M_r \times N_r$ submatrix of $C$, as shown at layer 7.
In OpenBLAS, matrix $C$ is scaled by $\beta$ before the loops,
so the multiply-accumulate operation at layer 7 need not to scale $C$ again.
Tiling factors $N_c$, $K_c$, $M_c$, $N_r$ and $M_r$ are carefully selected so that
matrices on each level fit into a certain level in the memory hierachy.
Equation~(\ref{eq:constraints.reg})--(\ref{eq:constraints.l3}) show
the constraints on these tiling factors.
$es$ means the size of matrix element, e.g. 8B for double precision floating-point numbers,
and $c_l$ means the capacity of registers or caches on level $l$.
First, by (\ref{eq:constraints.reg}), $M_r$ and $N_r$ are constrained that $M_r$ elements from $A$,
$N_r$ elements from $B$ and $M_r \times N_r$ elements from $C$ can fit into registers
(the psudo level-0 cache).
Then, by (\ref{eq:constraints.l1}), $K_c$ is constrained that $B_4$ ($N_r \times K_c$),
$A_4$ ($M_r \times K_c$), and $A_4$'s counterpart in next iteration
at layer 5 fit into the L1 cache.
Then, by (\ref{eq:constraints.l2}), $M_c$ is constrained that $A_3$ ($M_c \times K_c$),
$B_3$ ($N_r \times K_c$), and $B_3$'s counterpart in next iteration
at layer 4 fit into the L2 cache.
At last, by (\ref{eq:constraints.l3}), $N_c$ is constrained that 
$B_2$ ($K_c \times N_c$) and $A_2$ ($M_c \times K_c$) fit into the L3 cache, if it exists.
Larger matrices on higher layers 1 -- 3 live in main memory.

\begin{eqnarray}
  es (M_r + N_r + M_r N_r) & \le & c_{0} \label{eq:constraints.reg}\\
  nt_{1} \cdot es (N_r K_c + 2 M_r K_c) & \le & c_{1} \label{eq:constraints.l1}\\
  nt_{2} \cdot es (M_c K_c + 2 N_r K_c) & \le & c_{2} \label{eq:constraints.l2}\\
  es (N_c K_c + nt_{3} \cdot M_c K_c)   & \le & c_{3} \label{eq:constraints.l3}
\end{eqnarray}

\begin{comment}
Table~\ref{tab:factors} shows the procedure choosing various tiling factors.
In each row, one or more tiling factors (column 1) is determined
by a size constraint (column 3) related to a certain level
in the memory hierachy (column 2).
\begin{table}
  \centering
  \caption{Procedure of choosing tiling factors}
  \label{tab:factors}
  \begin{tabular}{cccl}
    \toprule
    factor & memory & constraint \\
    \midrule
    $M_r$, $N_r$ & registers & $M_r + N_r + M_r N_r \le Regs$ \\
    $K_c$        & L1 cache  & $N_r K_c + 2 M_r K_c \le L1$ \\
    $M_c$        & L2 cache  & $M_c K_c + 2 N_r K_c \le L2$ \\
    $N_c$        & L3 cache  & $N_c K_c + M_c K_c \le L3$ \\
    \bottomrule
  \end{tabular}
\end{table}
\end{comment}

Goto~\cite{gotogemm} factors out the innermost three loops at layers 4 -- 6 for
computing $C_2\ = C_2 + \alpha A_2 B_2$ as an architecture-dependent kernel,
known as  GEBP (GEneral multiply of a Block of $A$ and a Panel of $B$).
It is worthy noting that before GEBP is called (at layer 3),
$A_1[ii:ii+M_c-1][:]$ and $B_1$ are packed into $A_2$ and $B_2$
in a special continous layout,
so consecutive memory access is ensured within the GEBP kernel.
GotoBLAS~\cite{gotoblas}, and its successor, OpenBLAS~\cite{openblas},
implement GEMM programs based on this factorization,
with GEBP highly optimized for the target processor (often coded in assembly).

As stated in Section~\ref{sec:intro}, an GEMM implementation
is made up by a kernel routine and an overall strategy.
In Figure.~\ref{fig:gemm}, GEBP surves as the kernel routine,
and the overall strategy is defined by the three outter loops at layers 1 -- 3.
Developer can control how to partition the workload by choosing
tile factors $M_c$, $N_c$ and $K_c$,
and how to schedule GEBP tasks by interchanging loop orders at layers 1 - 3.
Either the N loop at layer 1 or M loop at layer 3, or both of them,
can be parallelized in a multi-threaded context.
Specifically, the strategy in Figure.~\ref{fig:gemm} chooses a NKM loop order and
parallelizes the M loop at layer 3.
Different threads work on different parts of $A_1$
and share the same $B_1$.
As a result, there are multiple $A_2$ instances, one per thread,
and a single $B_2$ instances, shared by all threads.
To understand how the loop on layer 3 is parallelized on the 4-core processor, 
Figure.~\ref{fig:workload} shows the data accessing patterns of a single thread $T_1$.
While all color shaded submatrices are accessed by $T_1$,
only z-curve masked submatrices (a subset of color shaded ones)
are packed by the thread.
That is, each thread packs its own $A_2$ instance and 
$B_2$ is packed by all thread collabratively.
%% We will use the OpenBLAS GEMM implementation in Figure.~\ref{fig:gemm}
%% in discussion and evaluation throughout the paper.

\begin{figure}[t]
  \centering
  \includegraphics[width=.4\textwidth]{figures/workload}
  \caption{GEMM parallelization with 4 threads and data accessing patterns of thread $T_1$}
  \label{fig:workload}
\end{figure}

\subsection{Cache Conflicts in GEMM}\label{subsec:cache-conflicts}

For set-associative caches, cache misses can be categorized
into capacity misses and conflict misses.
While constraints (\ref{eq:constraints.reg}) -- (\ref{eq:constraints.l3})
serve to avoid capacity misses,
conflict misses can not be eliminated completely.
%% We use the term ``cache conflict'' to represent conflict miss events.

Recall Figure.~\ref{fig:gemm}, the packed matrix $A_2$
will be read $N_c/N_r$ times within a single GEBP execution,
and is expected to reside in L2 cache during all $N_c/N_r$ iterations at layer 4.
The packed matrix $B_4$ is similar that it is expected to
reside in L1 cache during all $M_c/M_r$ iterations at layer 5.
In general, even with carefully selected tile factors,
matrix data can still be evicted out for a few reasons.
For instance, data structures other than the packed matrices,
or program code (in case of a unified cache) may be fetched into a
cache set the matrix data lives in.
We call this kind of cache conflicts intra-thread cache conflicts
because it occurs within the execution of a single thread.
To deal with intra-thread conflicts,
the GEBP kernel repeatedly prefetch data from $A_4$ and $B_4$ to L1 cache
in every iteration at layer 6 before it actually reads them in case of
the data being evicted out accidentally.
As GEBP accesses the matrix data much more frequently than other
data structures, this repeated prefething stratecy
can remedy the penalty caused by intra-thread conflicts.

In contrary to intra-thread cache conflicts,
there exists another kind of cache conflicts,
inter-thread cache conflicts, on architectures with shared caches.
Suppose two working threads, $T_0$ and $T_1$, are running independently
on the 4-core processor in Figure.~\ref{fig:hierachy}
to accomplish a GEMM operation.
At time $t_s$, $T_0$ prefetches some data at address $addr_0$
to cache set $\varphi_{L2}(addr_0)$ of the shared L2 cache.
The data will be actually read at time $t_e$.
Then at some time $t_m$ ($t_s < t_m < t_e$),
$T_1$ prefetches data at addr $addr_1$ to L2 cache.
If $\varphi_L(addr_1)$ happens to be equal to $\varphi_2(addr_0)$,
data at $addr_1$ and $addr_0$ will be fetched to the same cache set.
As the shared L2 cache has a non-LRU policy,
data at $addr_0$ may suffer an eviction before it is actually read,
i.e. a conflict miss occurs.

In GEMM, the cache is utilized aggressively
that almost all cache space is expected to be occupied by packed matrices.
Prefetch instructions from different threads are executed repeatedly,
interleaved in an unpredictable manner.
So inter-thread cache conflicts described above
can occur frequently on non-LRU shared caches,
thus making the technique dealing with intra-thread cache conflicts helpless.
The situation goes worse with more threads sharing the same cache.

